# Enhanced CoT MCP Server Environment Variables
# Copy to .env and fill in your values

# =============================================================================
# LLM API Configuration
# =============================================================================

# Required: API key for your LLM provider
# Works with OpenAI, Azure OpenAI, Ollama, LM Studio, vLLM, or any OpenAI-compatible API
OPENAI_API_KEY=sk-your-api-key-here

# Optional: Custom API base URL for OpenAI-compatible endpoints
# Leave unset or empty to use default OpenAI API (https://api.openai.com/v1)
#
# Examples:
#   Azure OpenAI:     https://your-resource.openai.azure.com/openai/deployments/your-deployment
#   Ollama:           http://localhost:11434/v1
#   LM Studio:        http://localhost:1234/v1
#   vLLM:             http://localhost:8000/v1
#   OpenRouter:       https://openrouter.ai/api/v1
#   Together AI:      https://api.together.xyz/v1
#   Anthropic (shim): https://api.anthropic.com/v1
#   Local llama.cpp:  http://localhost:8080/v1
#
OPENAI_BASE_URL=

# Optional: Override model name from config.yaml
# Useful when using different providers with different model names
#
# Examples:
#   OpenAI:           gpt-4-turbo, gpt-4o, gpt-3.5-turbo
#   Ollama:           llama3.1, mistral, codellama
#   Azure:            your-deployment-name
#   Together:         meta-llama/Llama-3-70b-chat-hf
#
OPENAI_MODEL=

# =============================================================================
# Server Configuration
# =============================================================================

# Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Server settings (usually set in config.yaml instead)
# SERVER_HOST=localhost
# SERVER_PORT=8000
# SERVER_TRANSPORT=stdio
