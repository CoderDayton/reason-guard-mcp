# MatrixMind MCP Server Environment Variables
# Copy to .env and fill in your values (for local development)
# Or set these environment variables directly when running via uvx

# =============================================================================
# LLM API Configuration (Required)
# =============================================================================

# Required: API key for your LLM provider
# Works with OpenAI, Azure OpenAI, Ollama, LM Studio, vLLM, or any OpenAI-compatible API
OPENAI_API_KEY=sk-your-api-key-here

# Optional: Custom API base URL for OpenAI-compatible endpoints
# Leave unset or empty to use default OpenAI API (https://api.openai.com/v1)
#
# Examples:
#   Azure OpenAI:     https://your-resource.openai.azure.com/openai/deployments/your-deployment
#   Ollama:           http://localhost:11434/v1
#   LM Studio:        http://localhost:1234/v1
#   vLLM:             http://localhost:8000/v1
#   OpenRouter:       https://openrouter.ai/api/v1
#   Together AI:      https://api.together.xyz/v1
#   Anthropic (shim): https://api.anthropic.com/v1
#   Local llama.cpp:  http://localhost:8080/v1
#
# OPENAI_BASE_URL=

# Optional: Model name (default: gpt-4.1)
#
# Examples:
#   OpenAI:           gpt-4.1, gpt-4.1-mini, gpt-4o, o3-mini
#   Ollama:           llama3.3, qwen2.5:72b, mistral-large
#   Azure:            your-deployment-name
#   Together:         meta-llama/Llama-3.3-70B-Instruct-Turbo
#
# OPENAI_MODEL=gpt-4.1

# =============================================================================
# Model Configuration
# =============================================================================

# Embedding model for compression (default: Snowflake/snowflake-arctic-embed-xs)
# Uses HuggingFace models for local inference (CPU or GPU)
#
# Examples:
#   Snowflake/snowflake-arctic-embed-xs  - Fast, lightweight (default, ~90MB)
#   Snowflake/snowflake-arctic-embed-m   - Better quality (~440MB)
#   sentence-transformers/all-MiniLM-L6-v2 - Classic small model (~80MB)
#   sentence-transformers/all-mpnet-base-v2 - Higher quality (~420MB)
#   BAAI/bge-small-en-v1.5               - Good balance (~130MB)
#
# EMBEDDING_MODEL=Snowflake/snowflake-arctic-embed-xs

# Custom directory for embedding model cache (default: ~/.cache/matrixmind-mcp/models/)
# Models are downloaded on first use and cached for subsequent runs
# Useful for Docker volumes, shared caches, or custom storage locations
#
# EMBEDDING_CACHE_DIR=~/.cache/matrixmind-mcp/models/

# =============================================================================
# LLM Request Settings
# =============================================================================

# Request timeout in seconds (default: 60)
# LLM_TIMEOUT=60

# Max retry attempts for failed requests (default: 3)
# LLM_MAX_RETRIES=3

# Default sampling temperature (0.0-2.0)
# Lower values = more deterministic/focused responses
# Higher values = more creative/diverse responses
#
# Note: When not specified, the server uses model-specific optimal defaults:
#   - GPT-4/GPT-4o:    0.7 (general), 0.2 (coding variants)
#   - DeepSeek V3:     0.3 (API auto-scales temperature)
#   - DeepSeek R1:     0.6 (reasoning model)
#   - Qwen (standard): 0.7
#   - Qwen (thinking): 0.6
#   - Mistral Small:   0.15 (very low recommended)
#   - Mistral Large:   0.7
#   - Llama 4:         0.6
#   - Gemma 3:         1.0 (higher than typical)
#   - Claude:          0.7
#
# Setting this explicitly overrides the model-specific default.
# Recommended ranges by use case:
#   0.0-0.3: Factual Q&A, code generation, structured output
#   0.4-0.6: General reasoning, balanced responses
#   0.7-1.0: Creative writing, brainstorming
#
# Note: Some models (o1, o3) don't support temperature parameter
# LLM_TEMPERATURE=0.7

# =============================================================================
# Server Configuration
# =============================================================================

# Server name shown to MCP clients (default: MatrixMind-MCP)
# SERVER_NAME=MatrixMind-MCP

# Transport mode: stdio | http | sse (default: stdio)
# SERVER_TRANSPORT=stdio

# Host for http/sse transport (default: localhost)
# SERVER_HOST=localhost

# Port for http/sse transport (default: 8000)
# SERVER_PORT=8000

# =============================================================================
# Logging Configuration
# =============================================================================

# Log level: DEBUG | INFO | WARNING | ERROR (default: INFO)
# LOG_LEVEL=INFO

# Log format: json | text (default: text)
# LOG_FORMAT=text
