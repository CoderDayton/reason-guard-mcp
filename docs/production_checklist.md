# Production Deployment Checklist
## Enhanced Chain-of-Thought MCP Server with FastMCP 2.0

---

## **PRE-DEPLOYMENT VERIFICATION**

### Code Quality

- [ ] All tools have complete type hints
- [ ] All tools have comprehensive docstrings (shown to LLM)
- [ ] No bare except clauses (catch specific exceptions)
- [ ] All external calls wrapped in try-except
- [ ] JSON returned for all outputs (not plain text)
- [ ] Error messages are user-friendly
- [ ] No logging of sensitive data (API keys, tokens)
- [ ] All async functions properly declared
- [ ] No synchronous blocking operations in async code
- [ ] Config file has all required keys

### Tool-Specific Checks

**Compression Tool:**
- [ ] Handles empty/very short inputs gracefully
- [ ] Compression ratio validated (0.1-1.0)
- [ ] Returns JSON with all metrics
- [ ] Gracefully handles sentences with special characters
- [ ] Preserves order when requested
- [ ] Timeout protection for large documents

**MoT Reasoning Tool:**
- [ ] Matrix size validated (2-5 each dimension)
- [ ] Communication pattern recognized
- [ ] Handles LLM timeouts/failures
- [ ] Returns confidence 0-1
- [ ] Reasoning steps included (max 5 for output)
- [ ] Trace information structured

**Long Chain Tool:**
- [ ] Step count validated (1-50)
- [ ] Intermediate verification working
- [ ] Backtracking on error functional
- [ ] Timeout per step respected
- [ ] Final answer extraction robust

**Verification Tool:**
- [ ] Claims extraction works for various text
- [ ] Confidence calculation correct
- [ ] Handles context up to 20K tokens
- [ ] Returns yes/no + confidence, not ambiguous

### Security

- [ ] No SQL injection vectors (not using SQL, but principle applies)
- [ ] Input length limits enforced
- [ ] API key NOT in config file (environment variable)
- [ ] Error messages don't leak internals
- [ ] No hardcoded credentials
- [ ] Timeout on all external calls
- [ ] Rate limiting considerations (if multi-user)

### Performance

- [ ] Compression benchmark: <0.5s for 10K tokens
- [ ] MoT reasoning: <5 min for 3Ã—4 matrix
- [ ] Long chain: <30s per step average
- [ ] Verification: <2s for 10 claims
- [ ] Memory usage under control (monitor during tests)
- [ ] No memory leaks in repeated calls
- [ ] GPU/FAISS acceleration enabled if available

### Testing

- [ ] Unit tests passing: `pytest tests/`
- [ ] Integration tests passing: `pytest tests/test_integration.py`
- [ ] Edge cases tested:
  - [ ] Empty inputs
  - [ ] Very large inputs (20K+ tokens)
  - [ ] Special characters, emojis
  - [ ] Unicode edge cases
  - [ ] Malformed JSON inputs
- [ ] Error scenarios tested
- [ ] Timeout scenarios tested
- [ ] API key missing handled gracefully
- [ ] Network failures handled

---

## **DEPLOYMENT ENVIRONMENT**

### Dependencies

```bash
# Verify all dependencies installed
pip list | grep -E "fastmcp|torch|transformers|faiss|openai|sentence-transformers"

# Expected:
# fastmcp >= 2.0.0
# torch >= 2.0
# transformers >= 4.36
# faiss-gpu >= 1.7.4 (or faiss-cpu as fallback)
# openai >= 1.3.0
# sentence-transformers >= 2.2.0
```

### Environment Variables

```bash
# Verify all required env vars set
printenv | grep -E "OPENAI_API_KEY|LOG_LEVEL|SERVER"

# Checklist:
- [ ] OPENAI_API_KEY set
- [ ] LOG_LEVEL set (INFO for production)
- [ ] SERVER_HOST set (0.0.0.0 for Docker)
- [ ] SERVER_PORT set (8000 default)
- [ ] SERVER_TRANSPORT set (http for prod)
```

### Configuration

```yaml
# config.yaml verification checklist:
- [ ] All model names valid and accessible
- [ ] FAISS settings appropriate for GPU/CPU
- [ ] Tool timeouts reasonable (300s for MoT)
- [ ] LLM retry settings configured
- [ ] Logging level appropriate
- [ ] All required sections present
```

### Hardware

**Minimum:**
- 4 CPU cores
- 16GB RAM
- GPU optional (2GB VRAM minimum if using)

**Recommended:**
- 8+ CPU cores
- 32GB RAM
- GPU with 8GB+ VRAM (NVIDIA CUDA)

**Verify GPU availability:**
```bash
python -c "import torch; print(f'GPU available: {torch.cuda.is_available()}')"
```

---

## **DOCKER DEPLOYMENT**

### Image Build

```bash
# Build image
docker build -t enhanced-cot-mcp:latest .

# Verify build
docker images | grep enhanced-cot-mcp

# Test image locally
docker run -e OPENAI_API_KEY=$OPENAI_API_KEY enhanced-cot-mcp:latest
```

### Container Runtime

```bash
# Run container
docker run -d \
  --name cot-mcp \
  -p 8000:8000 \
  -e OPENAI_API_KEY=$OPENAI_API_KEY \
  -e LOG_LEVEL=INFO \
  --gpus all \
  --memory 32g \
  enhanced-cot-mcp:latest

# Verify running
docker ps | grep cot-mcp

# Check logs
docker logs -f cot-mcp

# Test health
curl http://localhost:8000/health
```

### Docker Compose

```bash
# Start stack
docker compose up -d

# Verify services
docker compose ps

# Check logs
docker compose logs -f cot-mcp

# Stop stack
docker compose down
```

---

## **MONITORING & OBSERVABILITY**

### Logging Setup

```python
# Verify structured logging in logs/
tail -f logs/enhanced-cot-mcp.log

# Expected format:
# {"timestamp": "...", "level": "INFO", "tool": "compress_prompt", "message": "..."}
```

### Metrics to Track

```python
# Key metrics for monitoring:
- [ ] Tools called per hour (throughput)
- [ ] Average tool execution time
- [ ] Error rate per tool
- [ ] Token usage per query
- [ ] Cost per query (OpenAI)
- [ ] GPU memory utilization
- [ ] Model load times
```

### Health Check

```bash
# Test all endpoints
for tool in compress_prompt matrix_of_thought_reasoning long_chain_of_thought verify_fact_consistency recommend_reasoning_strategy; do
  echo "Testing $tool..."
  curl -X POST http://localhost:8000/mcp/call_tool \
    -H "Content-Type: application/json" \
    -d "{\"tool\": \"$tool\", \"args\": {}}"
done
```

---

## **VERIFICATION TESTS**

### Smoke Tests (5 minutes)

```python
# tests/smoke_test.py
import asyncio
from fastmcp import Client

async def smoke_test():
    async with Client("src/server.py") as client:
        
        # Test 1: Compression
        result1 = await client.call_tool(
            "compress_prompt",
            {
                "context": "A " * 100,
                "question": "What is A?",
                "compression_ratio": 0.3
            }
        )
        assert result1, "Compression failed"
        print("âœ“ Compression OK")
        
        # Test 2: MoT
        result2 = await client.call_tool(
            "matrix_of_thought_reasoning",
            {
                "question": "What is 2+2?",
                "context": "Basic math",
                "matrix_rows": 2,
                "matrix_cols": 2
            }
        )
        assert result2, "MoT failed"
        print("âœ“ MoT OK")
        
        # Test 3: Long Chain
        result3 = await client.call_tool(
            "long_chain_of_thought",
            {
                "problem": "What is 2+2?",
                "num_steps": 3
            }
        )
        assert result3, "Long chain failed"
        print("âœ“ Long chain OK")
        
        # Test 4: Verify
        result4 = await client.call_tool(
            "verify_fact_consistency",
            {
                "answer": "2+2=4",
                "context": "2+2 equals 4"
            }
        )
        assert result4, "Verification failed"
        print("âœ“ Verification OK")
        
        print("\nâœ… All smoke tests passed!")

asyncio.run(smoke_test())
```

Run: `pytest tests/smoke_test.py`

### Load Test (10 minutes)

```python
# tests/load_test.py
import asyncio
import time
from fastmcp import Client

async def load_test(num_requests=100):
    """Simulate concurrent requests."""
    async with Client("src/server.py") as client:
        
        tasks = []
        start = time.time()
        
        for i in range(num_requests):
            task = client.call_tool(
                "compress_prompt",
                {
                    "context": f"Test document {i}. " * 50,
                    "question": f"What is test {i}?",
                    "compression_ratio": 0.3
                }
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        elapsed = time.time() - start
        
        successes = sum(1 for r in results if not isinstance(r, Exception))
        failures = len(results) - successes
        
        print(f"\nLoad Test Results:")
        print(f"  Requests: {num_requests}")
        print(f"  Successes: {successes}")
        print(f"  Failures: {failures}")
        print(f"  Time: {elapsed:.1f}s")
        print(f"  Throughput: {num_requests/elapsed:.1f} req/s")
        print(f"  Avg latency: {elapsed/num_requests*1000:.1f}ms")

asyncio.run(load_test(100))
```

Run: `pytest tests/load_test.py`

---

## **DEPLOYMENT SCENARIOS**

### Scenario 1: Local Development

```bash
# Start server
fastmcp run src/server.py

# In another terminal, test
python examples/basic_usage.py

# Expected: Tool calls work, output appears in logs
```

**Validation:**
- [ ] Server starts without errors
- [ ] All tools callable
- [ ] Logging visible
- [ ] No permission issues

### Scenario 2: Production HTTP Server

```bash
# Start server
python src/server.py  # Uses config.yaml: transport=http

# Test endpoint
curl http://localhost:8000/mcp/tools

# Expected: JSON list of available tools
```

**Validation:**
- [ ] HTTP server starts on correct port
- [ ] Tools endpoint responds
- [ ] Health check working
- [ ] HTTPS ready (use reverse proxy like nginx)

### Scenario 3: Docker Production

```bash
# Start with compose
docker compose up -d

# Monitor
docker compose logs -f

# Test
curl http://localhost:8000/mcp/tools

# Scale (if using orchestration)
docker compose up -d --scale cot-mcp=3
```

**Validation:**
- [ ] Container starts
- [ ] Port mapping correct
- [ ] Environment variables passed
- [ ] GPU access working
- [ ] Logs mounted
- [ ] Restart policy set

### Scenario 4: FastMCP Cloud

```bash
# Deploy
fastmcp deploy src/server.py --name enhanced-cot-mcp --public

# Get endpoint
fastmcp info enhanced-cot-mcp

# Test via HTTPS
curl https://enhanced-cot-mcp.fastmcp.cloud/mcp/tools

# Monitor
fastmcp logs enhanced-cot-mcp
```

**Validation:**
- [ ] Deployment successful
- [ ] HTTPS endpoint active
- [ ] Public access working
- [ ] Logs accessible
- [ ] Auto-scaling (if enabled)

---

## **ROLLBACK PROCEDURE**

If deployment fails:

```bash
# 1. Check recent logs
docker logs cot-mcp | tail -50

# 2. Verify configuration
cat config.yaml

# 3. Check dependencies
pip show fastmcp

# 4. Rollback to previous version
docker pull enhanced-cot-mcp:v1.0.0
docker run -d --name cot-mcp-rollback ...

# 5. If all else fails, restart from scratch
docker compose down
docker volume rm cot-mcp_logs  # Clear logs if needed
docker compose up -d
```

---

## **MONITORING DASHBOARD (Optional)**

If you want real-time monitoring:

```yaml
# Add to docker-compose.yml
services:
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
```

Access Grafana at `http://localhost:3000`

---

## **FINAL CHECKLIST BEFORE GO-LIVE**

- [ ] All code reviewed and tested
- [ ] Security audit completed
- [ ] Performance benchmarks met
- [ ] Documentation updated
- [ ] Runbook created (how to restart, scale, debug)
- [ ] Monitoring configured
- [ ] Logging configured
- [ ] Backups configured (if needed)
- [ ] Disaster recovery plan written
- [ ] Team trained on deployment
- [ ] Rollback plan documented
- [ ] Load testing passed
- [ ] Smoke tests passing
- [ ] All environment variables configured
- [ ] SSL/HTTPS configured (for production)
- [ ] Rate limiting configured (if public)
- [ ] Error alerting configured
- [ ] On-call rotation established
- [ ] Communication channels ready (Slack, etc)

---

## **POST-DEPLOYMENT**

### First 24 Hours

- [ ] Monitor logs continuously
- [ ] Check error rate (should be <1%)
- [ ] Verify response times (should be <5s avg)
- [ ] Test all tools manually
- [ ] Check GPU memory usage if applicable
- [ ] Monitor API costs (OpenAI)

### First Week

- [ ] Analyze usage patterns
- [ ] Identify any bottlenecks
- [ ] Collect feedback from users
- [ ] Fine-tune parameters based on real usage
- [ ] Check for memory leaks (uptime >24h)
- [ ] Verify backup/recovery procedures

### Ongoing

- [ ] Daily log review
- [ ] Weekly performance reports
- [ ] Monthly security updates
- [ ] Quarterly capacity planning
- [ ] Continuous optimization

---

**Deployment Complete! ðŸš€**

Your Enhanced Chain-of-Thought MCP server is now production-ready with:
- âœ… FastMCP 2.0 modern framework
- âœ… 4 optimized tools (compress, MoT, long chain, verify)
- âœ… Production-grade error handling
- âœ… Complete monitoring and logging
- âœ… Docker support
- âœ… FAISS GPU acceleration
- âœ… Comprehensive testing

Welcome to production! ðŸŽ‰
